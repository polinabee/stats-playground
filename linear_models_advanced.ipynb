{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"../../Data/bgsedsc_0.jpg\">\n",
    "$\\newcommand{\\bb}{\\boldsymbol{\\beta}}$\n",
    "$\\DeclareMathOperator{\\Gau}{\\mathcal{N}}$\n",
    "$\\newcommand{\\bphi}{\\boldsymbol \\phi}$\n",
    "$\\newcommand{\\bx}{\\boldsymbol{x}}$\n",
    "$\\newcommand{\\bu}{\\boldsymbol{u}}$\n",
    "$\\newcommand{\\by}{\\boldsymbol{y}}$\n",
    "$\\newcommand{\\whbb}{\\widehat{\\bb}}$\n",
    "$\\newcommand{\\hf}{\\hat{f}}$\n",
    "$\\newcommand{\\tf}{\\tilde{f}}$\n",
    "$\\newcommand{\\ybar}{\\overline{y}}$\n",
    "$\\newcommand{\\E}{\\mathbb{E}}$\n",
    "$\\newcommand{\\Var}{Var}$\n",
    "$\\newcommand{\\Cov}{Cov}$\n",
    "$\\newcommand{\\Cor}{Cor}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# High-dimensional predictive regression models - more advanced concepts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We load the relevant modules\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "import pandas as pd\n",
    "#seaborn is a module for figures\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The bias-variance tradeoff in Statistics and Machine Learning - little maths behind the picture\n",
    "\n",
    "The picture tells it all! (Taken from Bishop's book)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/bias_variance_bishop.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A little math: bias-variance tradeoff\n",
    "\n",
    "Suppose that the data are generated according to a mechanism: \n",
    "\n",
    "$$ y = h(\\bx) + \\epsilon$$\n",
    "\n",
    "where \n",
    "\n",
    "+ $\\bx \\sim p(\\bx)$ \n",
    "+ $\\E[\\epsilon] = 0$\n",
    "+ $\\Var(\\epsilon) = v$\n",
    "+ $\\Cor(\\bx,\\epsilon) = 0$\n",
    "\n",
    "Also, let \n",
    "\n",
    "$$b_n(\\bu) = \\E[\\hf_n(\\bu)]$$ \n",
    "\n",
    "$$c_n(\\bu) = \\Var(\\hf_n(\\bu))$$\n",
    "\n",
    "Then: \n",
    "\n",
    "$$MSE_n = \\E[(y^*-\\hf_n(\\bx^*))^2] = v + \\E[(b_n(\\bx^*) - h(\\bx^*))^2] + \\E[c_n(\\bx^*)]$$\n",
    "\n",
    "+ The first term relates to the prediction problem: our algorithm cannot affect it\n",
    "+ The other two relate to the algorithm:\n",
    "     + Algorithms that underfit are those with small third term (small variance) and large second term (high bias)\n",
    "     + Algorithms that overfit are those with small second term (small bias) and large third term (high variance)\n",
    "     + For linear model the story is simple and mathematics can back it up: adding features increases the variance, removing features might increase the bias\n",
    "+ The MSE balances bias and variance: we want **algorithms that can strike a good bias-variance tradeoff**! \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model selection criteria\n",
    "\n",
    "Model selection criteria balance in-sample fit with model complexity\n",
    "\n",
    "For example, one such criterion is the so-called Akaike Information Criterion (AIC). For linear models this is equivalent to another model selection criterion, the so-called Mallows's $C_p$. \n",
    "\n",
    "AIC provides an approximation to a quantity related to but not the same as $MSE_n$ - by doing a bias-correction to the in-sample mean squared error. \n",
    "\n",
    "For a statistical model that involves $p$ parameters and loss function which is the log-likelihood, the AIC takes the form: \n",
    "\n",
    "$$AIC(\\textrm{model}) = -{2 \\over n} \\textrm{(maximised log-lik of model)} + {2 \\over n} p$$\n",
    "\n",
    "When we deviate from maximum likelihood, the *effective number of parameters* - we will call this **degrees of freedom (df)** differs from $p$. In particular: \n",
    "\n",
    "+ Algorithms that have used the response data to select the model, they will have $df \\geq p$: effectively more than $p$ parameters have been used to come up with this particular model\n",
    "+ Algorithms that shrink the parameter values towards zero, they will have $df \\leq p$: effectively each parameter counts less - or even 0 if it is shrunk exactly to 0 \n",
    "\n",
    "Specifically to the case where there is a family of algorithms indexed by a tuning parameter $\\lambda$ (e.g. the regularizing parameter) and AIC is used to score each member of the family, the AIC formula becomes: \n",
    "\n",
    "$$AIC(\\lambda) = -{2 \\over n} \\textrm{(maximised log-lik of model for given }\\lambda) + {2 \\over n} df(\\lambda)$$\n",
    "\n",
    "where $df(\\lambda)$ are the degrees of freedom for that particular value of the tuning parameter. \n",
    "\n",
    "The name of the game is to come up with an estimate of $df(\\lambda)$ for the algorithm of interest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Degrees of freedom of lasso\n",
    "\n",
    "Rather surprisingly the degrees of freedom for a given value of $\\lambda$ used in a lasso algorithm is *unbiasedly estimated* by the resultant number of non-zero coefficients. \n",
    "\n",
    "For the sake of clarity, let \n",
    "$m_\\lambda$ be the size of the model chosen with $\\lambda$ regularizer value, and $\\lambda_m$ the *transition point* in the lasso path (from empty to full) at which the model size becomes $m$ \n",
    "\n",
    "The result for the lasso is that $m_\\lambda$ is an unbiased estimator of $df(\\lambda)$ - this is established in Zou, Hastie and Tibshirani (2007, Annals of Statistics)\n",
    "\n",
    "+ Lasso uses the response data to choose the model, hence we would expect $df(\\lambda) \\geq m_\\lambda$ \n",
    "+ On the other hand, lasso shrinks coefficients to zero hence we would expect $df(\\lambda) \\leq m_\\lambda$\n",
    "+ Magically, the two effects cancel out and $m_\\lambda$ is unbiased for $df(\\lambda)$\n",
    "\n",
    "Combined with the Gaussian likelihood, the AIC criterion for lasso becomes \n",
    "\n",
    "$$AIC(\\lambda) = {\\sum_i (y_i - \\hf(\\bx_i))^2 \\over n v} + {2 \\over n} m_\\lambda$$\n",
    "\n",
    "An important result is that it is only necessary to search among the transition points $\\lambda_1,\\lambda_2,\\ldots$ to identify the model that optimizes the AIC criterion "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation\n",
    "\n",
    "Recall that the ultimate goal in predictive measures of performance is to estimate the mean squared error, \n",
    "\n",
    "$$MSE_n = \\E[(y^*-\\hf_n(\\bx^*))^2]$$\n",
    "\n",
    "Leave-one-out CV provides an estimator of this quantity: recall that:\n",
    "\n",
    "+ it splits the $n$ available data in $n$ groups - lets call them *folds*\n",
    "+ using the data in $n-1$ folds as training, it measures squared error on the left-out fold\n",
    "+ it then averages the squared errors from the $n$ folds\n",
    "\n",
    "More generally, $K$-fold CV proceeds by: \n",
    "\n",
    "+ it splits the $n$ available data in $K$ folds\n",
    "+ using the data in $K-1$ folds as training, it measures average squared error on the left-out fold\n",
    "+ it then averages the squared errors from the $K$ folds\n",
    "\n",
    "\n",
    "Little math shows that the generic formula for the estimator of $MSE_n$ from $K$-fold CV is (for simplicity say $n = Km$, and let $K(i)$ denote the fold the $i$th observation belongs to)\n",
    "\n",
    "$$\n",
    "{1 \\over n} \\sum_i \\left ( y_i - \\hf_{n-m, -K(i)}(\\bx_i) \\right)^2 \n",
    "$$\n",
    "\n",
    "The bias-variance tradeoff is at work here too:\n",
    "\n",
    "+ Small $K$, hence large $m$, is good since the estimate of mean squared error in each batch is based on more independent observations, hence *smaller variance*\n",
    "+ Small $K$, hence $n-m\\ll n$, is bad since it means that we are really evaluating $\\hf_{n-m}$ which could be significantly different from $\\hf_{n}$, hence *larger bias*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a very simple example, that of evaluating the regression model with intercept only with loss function given by the log-likelihood, we can do the calculations explicitly, and compute the mean squared error of the CV estimator as a function of $K$ - it also depends on second and fourth moments of the data generating process \n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td> Gaussian-tailed data </td>\n",
    "        <td> heavy-tailed data </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "    <td>  <img src=\"../images/mse_cv_gauss.png\" > </td>\n",
    "    <td> <img src=\"../images/mse_cv_student.png\"> </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "In these examples the minimum is around 3-5 fold, which results in leaving aside 20-30% of the data for testing. \n",
    "\n",
    "The story about the optimal choice of $K$ is neither straightforward nor conclusive. Alternative methods, such as the bootstrap, are also competitors to CV for obtaining estimates of prediction error "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
